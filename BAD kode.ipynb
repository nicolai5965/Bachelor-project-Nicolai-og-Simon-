{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e10272-c099-45b0-bacb-314991c4c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from IPython.display import display_html\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc6c35b-80ab-4131-a3db-b960b4c17594",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_called = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2c9ee-74f4-4e6f-b2fa-910063c60947",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac4117b-38ef-4cc1-8bfe-cb084f8b83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_import_100_sample(events_called):\n",
    "    hits = []\n",
    "    cells = []\n",
    "    particles = []\n",
    "    truth = []\n",
    "\n",
    "    # append datasets into the list\n",
    "    for i in range(0,events_called):\n",
    "        if i < 10:\n",
    "            temp_df = pd.read_csv('train_100_events/event00000100%d-hits.csv' % i)\n",
    "            #c_c = pd.read_csv('train_100_events/event00000100%d-cells.csv'% i )\n",
    "            p_c = pd.read_csv('train_100_events/event00000100%d-particles.csv'% i)\n",
    "            t_c = pd.read_csv('train_100_events/event00000100%d-truth.csv'% i)\n",
    "            hits.append(temp_df)\n",
    "            #cells.append(c_c)\n",
    "            particles.append(p_c)\n",
    "            truth.append(t_c)\n",
    "        if i >= 10:\n",
    "            temp_df = pd.read_csv('train_100_events/event0000010%d-hits.csv' % i)\n",
    "            #c_c = pd.read_csv('train_100_events/event0000010%d-cells.csv'% i )\n",
    "            p_c = pd.read_csv('train_100_events/event0000010%d-particles.csv'% i)\n",
    "            t_c = pd.read_csv('train_100_events/event0000010%d-truth.csv'% i)\n",
    "            #cells.append(c_c)\n",
    "            hits.append(temp_df)\n",
    "            particles.append(p_c)\n",
    "            truth.append(t_c)\n",
    "    return cells , hits , particles , truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc52b53b-3bfa-4242-8bcb-f752d7eb3407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cells_all , hits_all , particles_all , truth_all = func_import_100_sample(events_called)\n",
    "data_detectors = pd.read_csv(r\"detectors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a9001a-7721-4f43-9e85-22bb82581e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hits_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c44f6-5bc1-4703-94b1-62f0e80608cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Making functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f54992-bc13-4b95-b4a5-de85983ac33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cleaning_data( hits, particles , truth, pt_cut_start, pt_cut_end):\n",
    "    #Finding all hit_id that is noice, to use in other files for removing nocie in them.\n",
    "    def noice(truth):\n",
    "        truth_hit_id_noice = [truth.hit_id[i] for i in range(len(truth)) if truth.particle_id[i] == 0]\n",
    "        return truth_hit_id_noice\n",
    "    truth_hit_id_noice_list = noice(truth)\n",
    "    #Removing all the noice in the truth file:\n",
    "    truth_zero_noice = truth.drop(truth.index[truth['particle_id'] == 0]).reset_index()\n",
    "    #Removing all the data where the nhits is >=3 :\n",
    "    particles_zero_noice = particles.drop(particles.index[particles['nhits'] <= 3]).reset_index()\n",
    "    #Sorting the particles\n",
    "    particles_zero_noice_sorted_unique = particles_zero_noice.sort_values(by = \"particle_id\",ascending=True)\n",
    "    #Making a function that can remove all row that has a value in a list\n",
    "    def FRBV(file_name, column_name, list_of_values):\n",
    "        return file_name[~file_name[column_name].isin(list_of_values)]\n",
    "    # #Removing all the noice in the cells file:\n",
    "    # cells_zero_noice = FRBV(cells , \"hit_id\" , truth_hit_id_noice_list).reset_index()\n",
    "    #Removing all the noice in the hits file:\n",
    "    hits_zero_noice = FRBV(hits , \"hit_id\" , truth_hit_id_noice_list).reset_index()\n",
    "    #Making a function that can remove all the data, that has a nhits over 7.\n",
    "    def nhit_over_7(data):\n",
    "        data = [data.particle_id[i] for i in range(len(data)) if data.nhits[i] > 7]\n",
    "        return data\n",
    "    #Removing all the data where nhits is less then 7\n",
    "    particle_id_with_nhits_over_7 = nhit_over_7(particles_zero_noice_sorted_unique)\n",
    "    #Removing all the data where nhits is over then 7\n",
    "    particle_id_with_nhits_lees_7 = FRBV(particles_zero_noice_sorted_unique , \"particle_id\" , particle_id_with_nhits_over_7).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    #Removing all the data where a particle_id has more then 7 nhits.\n",
    "    truth_zero_noice_nhits_lees_7 = FRBV(truth_zero_noice , \"particle_id\" , particle_id_with_nhits_over_7).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    #Making a function that can make a list of the hit_ids that has a weight of 0.\n",
    "    def weight_equle_0(data):\n",
    "        data = [data.hit_id[i] for i in range(len(data)) if data.weight[i] == 0]\n",
    "        return data\n",
    "    #Using the weight_equle_0 function to make a list of hit_id´s that has a weight equle 0\n",
    "    truth_weight_0_list = weight_equle_0(truth_zero_noice_nhits_lees_7)\n",
    "    #Using the list of hit_id´s that has a weight equle 0, to remove the rows in truth that has that hit_id.\n",
    "    truth_zero_noice_nhits_lees_7_weight_0 = FRBV(truth_zero_noice_nhits_lees_7,\"hit_id\",truth_weight_0_list).reset_index().drop(\"index\",axis = 1)\n",
    "    \n",
    "    #Removing the data where the particle_id has less then 7 nhits.\n",
    "    truth_zero_noice_nhits_over_7 = FRBV(truth_zero_noice , \"particle_id\" ,truth_zero_noice_nhits_lees_7.particle_id).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    #Removing the data where the hit_id has less the 7 nhits.\n",
    "    hits_zero_noice_nhits_lees_7 = FRBV(hits_zero_noice, \"hit_id\",truth_zero_noice_nhits_over_7[\"hit_id\"]).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    #Making a list of all the particle_ids that has over 3 nhits\n",
    "    particles_id_over_3 = [particles.particle_id[i] for i in range(len(particles)) if particles.nhits[i] > 3]\n",
    "    #Removing all the data in truth that has less the 3 nhits. Used for later in cells and hits.  \n",
    "    truth_zero_noice_over_3 = FRBV(truth_zero_noice,\"particle_id\",particles_id_over_3).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "    #Removing all the data in hits that has over the 3 nhits.\n",
    "    hits_zero_noice_nhits_lees_7_over_3_with_weight_0 = FRBV(hits_zero_noice_nhits_lees_7,\"hit_id\",truth_zero_noice_over_3.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "    #Making a list of hit_id that is not in truth but is in hits\n",
    "    hit_id_in_hits_but_not_in_truth = FRBV(hits_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",truth_zero_noice_nhits_lees_7_weight_0.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "    #Using that hits not in truth, and then removing them from hits. \n",
    "    hits_zero_noice_nhits_lees_7_over_3_without_weight_0 = FRBV(hits_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",hit_id_in_hits_but_not_in_truth.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "\n",
    "#     #Removing the data where the hit_id has less the 7 nhits.\n",
    "#     cells_zero_noice_nhits_lees_7 = FRBV(cells_zero_noice, \"hit_id\",truth_zero_noice_nhits_over_7[\"hit_id\"]).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)\n",
    "#     #Removing all the data in cells that has over the 3 nhits.\n",
    "#     cells_zero_noice_nhits_lees_7_over_3_with_weight_0 = FRBV(cells_zero_noice_nhits_lees_7,\"hit_id\",truth_zero_noice_over_3.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "#     #Making a list of hit_id that is not in truth but is in cells\n",
    "#     hit_id_in_cells_but_not_in_truth = FRBV(cells_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",truth_zero_noice_nhits_lees_7_weight_0.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "#     #Using that cells hit_id not in truth, and then removing them from cells.\n",
    "#     cells_zero_noice_nhits_lees_7_over_3_without_weight_0 = FRBV(cells_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",hit_id_in_cells_but_not_in_truth.hit_id).reset_index().drop(\"index\",axis = 1)\n",
    "    \n",
    "    #Putting all the data from truth into hits:\n",
    "    hits_merge = hits_zero_noice_nhits_lees_7_over_3_without_weight_0.merge(truth_zero_noice_nhits_lees_7_weight_0, how='left', on='hit_id')\n",
    "    #Making a same layer filter \n",
    "    def same_layer_filter(hits_new):\n",
    "        hits_long_layer_filtert = hits_new.drop_duplicates(subset = [\"particle_id\",\"volume_id\", \"layer_id\"])\n",
    "        return hits_long_layer_filtert\n",
    "    hits_long_layer_filtert = same_layer_filter(hits_merge)\n",
    "    #Making a function that can show the removed particles from same layer filter.\n",
    "    def Remove_Elements(data, thrsshold):\n",
    "        counted = Counter(data)\n",
    "        temp_lst = []\n",
    "        for i in counted:\n",
    "            if counted[i] < thrsshold:\n",
    "                temp_lst.append(i)\n",
    "        res_lst = []\n",
    "        for i in data:\n",
    "            if i not in temp_lst:\n",
    "                res_lst.append(i)\n",
    "        data = [i for i in data if counted[i] >= thrsshold]\n",
    "        return data , temp_lst , res_lst\n",
    "    thrsshold = 4\n",
    "    hits_filtert , particles_nhits_less_4 , particles_nhits_over_4 = Remove_Elements(hits_long_layer_filtert[\"particle_id\"],thrsshold)\n",
    "    #Removing the particles from the same layer filter in hits_merge and particle file\n",
    "    hits_merge_filtered = FRBV(hits_long_layer_filtert ,\"particle_id\", particles_nhits_less_4).reset_index().drop(\"index\",axis = 1)\n",
    "    particles_merge_filtered = FRBV(particle_id_with_nhits_lees_7 ,\"particle_id\", particles_nhits_less_4).reset_index().drop(\"index\",axis = 1)\n",
    "\n",
    "    #Calculating the pt of a particle:\n",
    "    Pt = []\n",
    "    for i in (range(len(particles_merge_filtered))):\n",
    "        func = (particles_merge_filtered.px[i]**2+particles_merge_filtered.py[i]**2)**(1/2)\n",
    "        Pt.append([particles_merge_filtered.particle_id[i],func])\n",
    "    #Making the pt filter loop\n",
    "    Pt_1GeV = []\n",
    "    for i in range(len(Pt)):\n",
    "        if Pt[i][1] >= pt_cut_start and Pt[i][1] <= pt_cut_end: \n",
    "            Pt_1GeV.append(Pt[i])\n",
    "    #Finding the particles from the pt filter \n",
    "    particles_over_Pt_1GeV = []\n",
    "    for i in range(len(Pt_1GeV)):\n",
    "        particles_over_Pt_1GeV.append(Pt_1GeV[i][0])\n",
    "    # Making a function that keeps values that is in list, and removes the ones that is not. \n",
    "    def FCBV(file_name, column_name, list_of_values):\n",
    "        return file_name[file_name[column_name].isin(list_of_values)]\n",
    "    #Removing the rows from the pt filter.  \n",
    "    particles_over_Pt_1GeV_filtered = FCBV(particles_merge_filtered ,\"particle_id\", particles_over_Pt_1GeV).reset_index().drop(\"index\",axis = 1)\n",
    "    hits_merge_over_Pt_1GeV_filtered = FCBV(hits_merge_filtered ,\"particle_id\", particles_over_Pt_1GeV).reset_index().drop(\"index\",axis = 1)\n",
    "    return  hits_merge_over_Pt_1GeV_filtered , particles_over_Pt_1GeV_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2735ef1d-d3ab-4d08-a880-b04ce29cf268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Amount of nodes: 31623 Amount of different particles: 6457\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hits_merge_all = []\n",
    "for i in range(len(hits_all)):\n",
    "    hits_merge_all.append(func_cleaning_data(hits_all[i],particles_all[i],truth_all[i],1,3))\n",
    "\n",
    "print(len(hits_merge_all))\n",
    "    \n",
    "lenght_all_nodes_combined = []\n",
    "for i in range(len(hits_merge_all)):\n",
    "    lenght_all_nodes_combined.append(len(hits_merge_all[i][0]))\n",
    "lenght_all_particles_combined = []\n",
    "for i in range(len(hits_merge_all)):\n",
    "    lenght_all_particles_combined.append(len(hits_merge_all[i][1]))\n",
    "print(\"Amount of nodes:\",sum(lenght_all_nodes_combined),\"Amount of different particles:\",sum(lenght_all_particles_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b963ee-d989-4fb3-9f7b-1f55b5554c2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6d3016-8a4f-4557-9683-691ed6c5541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_and_edges(data,x,y,z):\n",
    "    def transform_and_merge(data,x,y,z):\n",
    "        def car_to_cyl_cood(x,y,z):\n",
    "            r = np.sqrt(x**2 + y**2)\n",
    "            phi = np.arctan2(y,x)\n",
    "            z = z\n",
    "            return r , phi , z \n",
    "        r_hits , phi_hits , z_hits = car_to_cyl_cood(x,y,z)\n",
    "        data[\"phi\"] = phi_hits\n",
    "        data[\"r\"] = r_hits\n",
    "        return data , r_hits , phi_hits , z_hits\n",
    "\n",
    "    hits_merge , r_hits , phi_hits , z_hits = transform_and_merge(data,data.x , data.y , data.z)\n",
    "    def car_to_sphe_cood(r,z):\n",
    "        theta = np.arctan2(r,z)\n",
    "        return theta\n",
    "\n",
    "    theta = car_to_sphe_cood(r_hits,z_hits)\n",
    "    eta = -np.log(np.tan(theta/2))\n",
    "    rr = np.array(hits_merge.r)\n",
    "    zz = np.array(hits_merge.z)\n",
    "    pp = np.array(hits_merge.phi)\n",
    "    dpp = []\n",
    "    dpdr = []\n",
    "    delta_phi = []\n",
    "    delta_z = []\n",
    "    delta_r = []\n",
    "    delta_eta = []\n",
    "    node_to_node_feat = []\n",
    "    for i, idx in enumerate(range(len(pp))):\n",
    "        for k, kdx in enumerate(range(len(rr))):\n",
    "            dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
    "            dr = rr[k]-rr[i]\n",
    "            z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n",
    "            if abs(dpdr) < 0.05 and abs(z0) < 249 and dr > 0:\n",
    "                dpp.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1,dpdr, idx, kdx, hits_merge.hit_id[idx],hits_merge.hit_id[kdx],abs(dr),hits_merge.z[idx],hits_merge.z[kdx],hits_merge.r[idx],hits_merge.r[kdx],hits_merge.phi[idx],hits_merge.phi[kdx]])\n",
    "                node_to_node_feat.append([hits_merge.z[idx],hits_merge.z[kdx],hits_merge.r[idx],hits_merge.r[kdx],hits_merge.phi[idx],hits_merge.phi[kdx]])\n",
    "\n",
    "    dp_over_dr_TF = pd.DataFrame(dpp)\n",
    "    dp_over_dr_TF.columns = ['Y_k' , \"dpdr\", 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\"]\n",
    "\n",
    "    dp_over_dr_True = []\n",
    "    for i in range(len(dpp)):\n",
    "        if dpp[i][0] == 1:\n",
    "            dp_over_dr_True.append(dpp[i])\n",
    "    dp_over_dr_True = pd.DataFrame(dp_over_dr_True)\n",
    "    dp_over_dr_True.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\"]\n",
    "    dp_over_dr_True = dp_over_dr_True.drop_duplicates(subset = \"dpdr\",ignore_index = True)\n",
    "\n",
    "    dp_over_dr_False = []\n",
    "    for i in range(len(dpp)):\n",
    "        if dpp[i][0] == 0:\n",
    "            dp_over_dr_False.append(dpp[i])\n",
    "    dp_over_dr_False = pd.DataFrame(dp_over_dr_False)\n",
    "    dp_over_dr_False.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\"]\n",
    "    dp_over_dr_False = dp_over_dr_False.drop_duplicates(subset = \"dpdr\",ignore_index = True)\n",
    "\n",
    "    def edges_feats():\n",
    "        for i, idx in enumerate(range(len(pp))):\n",
    "            for k, kdx in enumerate(range(len(rr))):\n",
    "                dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
    "                dr = rr[k]-rr[i]\n",
    "                dphi = pp[k]- pp[i]\n",
    "                dz = zz[k]-zz[i]\n",
    "                deta = eta[k]-eta[i]\n",
    "                z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n",
    "                if abs(dpdr) < 0.05 and abs(z0) < 249 and dr > 0:\n",
    "                    delta_r.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1, abs(dr) , idx, kdx])\n",
    "                    delta_phi.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1,abs(dphi), idx, kdx])\n",
    "                    delta_z.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1,abs(dz), idx, kdx])\n",
    "                    delta_eta.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1,abs(deta), idx, kdx])\n",
    "\n",
    "        def only_true(data , x):\n",
    "            liste_1 = pd.DataFrame(data)\n",
    "            liste_1.columns = ['Y_k' , x, 'node_1', 'node_2']\n",
    "            #liste_1 = liste_1.drop_duplicates(subset = [x],ignore_index = True)\n",
    "            liste_2 = []\n",
    "            for i in range(len(data)):\n",
    "                if data[i][0] == 1:\n",
    "                    liste_2.append(data[i])\n",
    "            liste_2 = pd.DataFrame(liste_2)\n",
    "            liste_2.columns = ['Y_k' , x , 'node_1', 'node_2']\n",
    "            liste_2 = liste_2.drop_duplicates(subset = [x],ignore_index = True)\n",
    "            return liste_1 , liste_2\n",
    "\n",
    "        delta_r = only_true(delta_r,\"dr\")\n",
    "        delta_phi = only_true(delta_phi,\"dphi\")\n",
    "        delta_z = only_true(delta_z,\"dz\")\n",
    "        delta_eta = only_true(delta_eta,\"deta\")\n",
    "\n",
    "        def delta_R(delta_eta,delta_phi):\n",
    "            delta_R = []    \n",
    "            for i in range(len(delta_eta)):\n",
    "                dR = (delta_eta.deta[i]**2 + delta_phi.dphi[i]**2)**(1/2)\n",
    "                delta_R.append(dR)\n",
    "            delta_R = pd.DataFrame(delta_R)\n",
    "            delta_R.columns = ['delta_R']\n",
    "            return delta_R\n",
    "\n",
    "        return delta_r , delta_phi , delta_z , delta_eta\n",
    "\n",
    "    #Making the datafram that contains the nodes ids and the nodes features. \n",
    "    nodes_id_plus_features = hits_merge[[\"hit_id\",\"z\",\"r\",\"phi\"]].copy()\n",
    "    \n",
    "    return nodes_id_plus_features , dp_over_dr_True , dp_over_dr_False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cef0fc-785c-4b9a-984a-e42a59143d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_12372/2789436489.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_12372/2789436489.py:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_12372/2789436489.py:32: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_12372/2789436489.py:34: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hits_merge_all = [nodes_and_edges(hits_merge_all[i][0] , hits_merge_all[i][0].x , hits_merge_all[i][0].y , hits_merge_all[i][0].z) for i in range(len(hits_merge_all))] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d275bb-48b9-40ff-8427-957cfb4c9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49838\n",
      "24673\n",
      "25165\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def REWMH(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_True = data[1].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_True = data_sorted_True.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    data_sorted_False = data[2].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_False = data_sorted_False.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    True_and_false = data_True.append(data_False, ignore_index=True)\n",
    "    return True_and_false\n",
    "\n",
    "def REWMHT(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_True = data[1].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_True = data_sorted_True.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    return data_True\n",
    "\n",
    "def REWMHF(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_False = data[2].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_False = data_sorted_False.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    return data_False\n",
    "\n",
    "All_edges_TF = [REWMH(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "All_edges_T = [REWMHT(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "All_edges_F = [REWMHF(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "\n",
    "\n",
    "All_edges_TF_combined = pd.concat(All_edges_TF, ignore_index=True)\n",
    "All_edges_T_combined = pd.concat(All_edges_T, ignore_index=True)\n",
    "All_edges_F_combined = pd.concat(All_edges_F, ignore_index=True)\n",
    "\n",
    "print(len(All_edges_TF_combined))\n",
    "print(len(All_edges_T_combined))\n",
    "print(len(All_edges_F_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6c696a-2e52-483a-a97f-0ec6bcf1056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 22628, 1: 22226})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44854"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "Y = All_edges_TF_combined.Y_k\n",
    "X = All_edges_TF_combined[[\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\"]].copy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "print(Counter(Y_train))\n",
    "display(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92227a0-2980-403d-a640-598c05c3f7e0",
   "metadata": {},
   "source": [
    "# Begining to make a GNN, starting with MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4980f8f4-6cc0-4182-8cef-c47191c000a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = MLPClassifier(hidden_layer_sizes = (200,100),random_state=1, max_iter = 300)\n",
    "model.fit(X_train,Y_train)\n",
    "prop_train = model.predict(X_train)\n",
    "score_train = model.score(X_train,Y_train)\n",
    "True_predicted_train = Counter(prop_train)[1]\n",
    "False_predicted_train = Counter(prop_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b36c7ad-789f-4468-ab92-484e7506f62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44854\n",
      "True_input vs True_output...: 22226 vs 2466\n",
      "False_input vs False_output...: 22628 vs 2518\n",
      "96.76 %\n"
     ]
    }
   ],
   "source": [
    "print(len(prop_train))\n",
    "print(\"True_input vs True_output...:\",Counter(Y_train)[1],\"vs\",True_predicted_test)\n",
    "print(\"False_input vs False_output...:\",Counter(Y_train)[0],\"vs\",False_predicted_test)\n",
    "print(round(score_train,4)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27941b9e-22a2-4e9e-a0d9-780b654cfdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984\n",
      "True_input vs True_output...: 2447 vs 2466\n",
      "False_input vs False_output...: 2537 vs 2518\n",
      "96.41 %\n"
     ]
    }
   ],
   "source": [
    "prop_test = model.predict(X_test)\n",
    "score_test = model.score(X_test,Y_test)\n",
    "True_predicted_test = Counter(prop_test)[1]\n",
    "False_predicted_test = Counter(prop_test)[0]\n",
    "print(len(prop_test))\n",
    "print(\"True_input vs True_output...:\",Counter(Y_test)[1],\"vs\",True_predicted_test)\n",
    "print(\"False_input vs False_output...:\",Counter(Y_test)[0],\"vs\",False_predicted_test)\n",
    "print(round(score_test,4)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4248abe-4613-4630-b460-ab6ab8e1a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = []\n",
    "for i in range(len(prob1)):\n",
    "    if prob1[i] == 1:\n",
    "        X_true.append(X1[i])\n",
    "\n",
    "\n",
    "edges = np.array(X_true)\n",
    "xyz = edges[:,2:8]\n",
    "plt.figure(69, figsize= (10,10))\n",
    "for i in range(len(xyz)):\n",
    "    px = np.array([xyz[i,0],xyz[i,1]])\n",
    "    py = np.array([xyz[i,4], xyz[i,5]])\n",
    "    plt.scatter(px,py, s = 10, color = 'black')\n",
    "    plt.plot(px,py, '--', alpha = 0.1, color = 'b')\n",
    "    #plt.xlim(35,100)\n",
    "    #plt.ylim(15,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4defc32d-e430-4843-a39a-f05dfadf3296",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kigger på data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be89cb-e1b8-435a-904e-b284a3a6c6e4",
   "metadata": {},
   "source": [
    "### Ploting data pre GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32cb3-997e-487c-a41a-ee1b4ec24519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_data(data_rough,data_clean):\n",
    "    XYZ_rough = [[],[],[]]\n",
    "    XYZ_clean = [[],[],[]]\n",
    "    for i, idx in enumerate(range(len(data_rough.x))):\n",
    "        if abs(data_rough.z[i]) < 2000:\n",
    "            XYZ_rough[0].append((data_rough.x[idx]))\n",
    "            XYZ_rough[1].append((data_rough.y[idx]))\n",
    "            XYZ_rough[2].append((data_rough.z[idx]))\n",
    "    print(len(XYZ_rough[0]), len(XYZ_rough[1]), len(XYZ_rough[2]))\n",
    "\n",
    "    for i, idx in enumerate(range(len(data_clean.x))):\n",
    "        if abs(data_clean.z[i]) < 2000:\n",
    "            XYZ_clean[0].append((data_clean.x[idx]))\n",
    "            XYZ_clean[1].append((data_clean.y[idx]))\n",
    "            XYZ_clean[2].append((data_clean.z[idx]))\n",
    "    print(len(XYZ_clean[0]), len(XYZ_clean[1]), len(XYZ_clean[2]))\n",
    "    return XYZ_rough , XYZ_clean\n",
    "\n",
    "XYZ_rough , XYZ_clean = plotting_data(hits_train_100,hits_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1cdaa-000c-452a-b21d-1068074182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom = 2000\n",
    "figsize = 5\n",
    "alpha = 0.35\n",
    "\n",
    "fig = plt.figure(1, figsize = (figsize,figsize))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(XYZ_rough[0],XYZ_rough[1],XYZ_rough[2], c = XYZ_rough[0], alpha = alpha)\n",
    "plt.xlim(-zoom,zoom)\n",
    "plt.ylim(-zoom,zoom)\n",
    "ax.set_zlim(-zoom,zoom)\n",
    "\n",
    "\n",
    "fig = plt.figure(2, figsize = (figsize,figsize))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(XYZ_clean[0],XYZ_clean[1],XYZ_clean[2], c = XYZ_clean[0] , alpha = alpha)\n",
    "plt.xlim(-zoom,zoom)\n",
    "plt.ylim(-zoom,zoom)\n",
    "ax.set_zlim(-zoom,zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543df60-0906-49be-bb89-c81c054a89db",
   "metadata": {},
   "source": [
    "### Looking at the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a23d2-70e8-4ddc-9d4a-39ff218c2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_detectors.cx\n",
    "y = data_detectors.cy\n",
    "z = data_detectors.cz\n",
    "fig = plt.figure(5, figsize = (10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(z , y ,x, c = y, alpha = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
