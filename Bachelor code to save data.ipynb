{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af4b7c8-1a66-4568-a7d3-9c4c91dad5f6",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c510ca-9852-411c-a5ac-50aca039b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from IPython.display import display_html\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b1d46-88be-4085-b80e-3d4c0940a8bb",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dcae0a4-c3df-418a-96eb-4e3b7c8b2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_import_100_sample(events_called):\n",
    "    hits = []\n",
    "    cells = []\n",
    "    particles = []\n",
    "    truth = []\n",
    "    for i in range(0,events_called):# append datasets into the list\n",
    "        if i < 10:\n",
    "            temp_df = pd.read_csv('train_100_events/event00000100%d-hits.csv' % i)\n",
    "            #c_c = pd.read_csv('train_100_events/event00000100%d-cells.csv'% i )\n",
    "            p_c = pd.read_csv('train_100_events/event00000100%d-particles.csv'% i)\n",
    "            t_c = pd.read_csv('train_100_events/event00000100%d-truth.csv'% i)\n",
    "            hits.append(temp_df)\n",
    "            #cells.append(c_c)\n",
    "            particles.append(p_c)\n",
    "            truth.append(t_c)\n",
    "        if i >= 10:\n",
    "            temp_df = pd.read_csv('train_100_events/event0000010%d-hits.csv' % i)\n",
    "            #c_c = pd.read_csv('train_100_events/event0000010%d-cells.csv'% i )\n",
    "            p_c = pd.read_csv('train_100_events/event0000010%d-particles.csv'% i)\n",
    "            t_c = pd.read_csv('train_100_events/event0000010%d-truth.csv'% i)\n",
    "            #cells.append(c_c)\n",
    "            hits.append(temp_df)\n",
    "            particles.append(p_c)\n",
    "            truth.append(t_c)\n",
    "    return cells , hits , particles , truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014affa0-d353-4867-ab4b-8b837f77015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "events_called = 100\n",
    "cells_all , hits_all , particles_all , truth_all = func_import_100_sample(events_called)\n",
    "data_detectors = pd.read_csv(r\"detectors.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7003ae2-e70d-4c6f-842a-ed28a0355a02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function that filters the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4c1f36-81e8-43c7-a8f9-c73593c2367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def func_cleaning_data( hits, particles , truth, pt_cut_start, pt_cut_end):\n",
    " \n",
    "    def noice(truth):#Finding all hit_id that is noice, to use in other files for removing nocie in them.\n",
    "        truth_hit_id_noice = [truth.hit_id[i] for i in range(len(truth)) if truth.particle_id[i] == 0]\n",
    "        return truth_hit_id_noice\n",
    "    truth_hit_id_noice_list = noice(truth)\n",
    "    truth_zero_noice = truth.drop(truth.index[truth['particle_id'] == 0]).reset_index() #Removing all the noice in the truth file:\n",
    "    particles_zero_noice = particles.drop(particles.index[particles['nhits'] <= 3]).reset_index()#Removing all the data where the nhits is >=3 :\n",
    "    particles_zero_noice_sorted_unique = particles_zero_noice.sort_values(by = \"particle_id\",ascending=True)#Sorting the particles\n",
    "    def FRBV(file_name, column_name, list_of_values): #Making a function that can remove all row that has a value in a list\n",
    "        return file_name[~file_name[column_name].isin(list_of_values)]\n",
    "    hits_volume_filtered = hits.drop(hits[(hits.volume_id != 8) & (hits.volume_id != 13) & (hits.volume_id !=17)].index)#Removing all that are not in volume 8 , 13 and 17\n",
    "    hits_zero_noice = FRBV(hits_volume_filtered , \"hit_id\" , truth_hit_id_noice_list).reset_index()    #Removing all the noice in the hits file:\n",
    "    def nhit_over_7(data):    #Making a function that can remove all the data, that has a nhits over 7.\n",
    "        data = [data.particle_id[i] for i in range(len(data)) if data.nhits[i] >= 100]\n",
    "        return data\n",
    "    particle_id_with_nhits_over_7 = nhit_over_7(particles_zero_noice_sorted_unique)#Removing all the data where nhits is less then 7\n",
    "    particle_id_with_nhits_lees_7 = FRBV(particles_zero_noice_sorted_unique , \"particle_id\" , particle_id_with_nhits_over_7).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)#Removing all the data where nhits is over then 7\n",
    "    truth_zero_noice_nhits_lees_7 = FRBV(truth_zero_noice , \"particle_id\" , particle_id_with_nhits_over_7).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)#Removing all the data where a particle_id has more then 7 nhits.\n",
    "    def weight_equle_0(data):#Making a function that can make a list of the hit_ids that has a weight of 0.\n",
    "        data = [data.hit_id[i] for i in range(len(data)) if data.weight[i] == 0]\n",
    "        return data\n",
    "    truth_weight_0_list = weight_equle_0(truth_zero_noice_nhits_lees_7)#Using the weight_equle_0 function to make a list of hit_id´s that has a weight equle 0\n",
    "    truth_zero_noice_nhits_lees_7_weight_0 = FRBV(truth_zero_noice_nhits_lees_7,\"hit_id\",truth_weight_0_list).reset_index().drop(\"index\",axis = 1)#Using the list of hit_id´s that has a weight equle 0, to remove the rows in truth that has that hit_id.\n",
    "    truth_zero_noice_nhits_over_7 = FRBV(truth_zero_noice , \"particle_id\" ,truth_zero_noice_nhits_lees_7.particle_id).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)#Removing the data where the particle_id has less then 7 nhits.\n",
    "    hits_zero_noice_nhits_lees_7 = FRBV(hits_zero_noice, \"hit_id\",truth_zero_noice_nhits_over_7[\"hit_id\"]).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)#Removing the data where the hit_id has less the 7 nhits.\n",
    "    particles_id_over_3 = [particles.particle_id[i] for i in range(len(particles)) if particles.nhits[i] > 3]#Making a list of all the particle_ids that has over 3 nhits\n",
    "    truth_zero_noice_over_3 = FRBV(truth_zero_noice,\"particle_id\",particles_id_over_3).drop(\"index\",axis = 1).reset_index().drop(\"index\",axis = 1)#Removing all the data in truth that has less the 3 nhits. Used for later in cells and hits.  \n",
    "    hits_zero_noice_nhits_lees_7_over_3_with_weight_0 = FRBV(hits_zero_noice_nhits_lees_7,\"hit_id\",truth_zero_noice_over_3.hit_id).reset_index().drop(\"index\",axis = 1) #Removing all the data in hits that has over the 3 nhits.\n",
    "    hit_id_in_hits_but_not_in_truth = FRBV(hits_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",truth_zero_noice_nhits_lees_7_weight_0.hit_id).reset_index().drop(\"index\",axis = 1)  #Making a list of hit_id that is not in truth but is in hits\n",
    "    hits_zero_noice_nhits_lees_7_over_3_without_weight_0 = FRBV(hits_zero_noice_nhits_lees_7_over_3_with_weight_0,\"hit_id\",hit_id_in_hits_but_not_in_truth.hit_id).reset_index().drop(\"index\",axis = 1) #Using that hits not in truth, and then removing them from hits. \n",
    "    hits_merge = hits_zero_noice_nhits_lees_7_over_3_without_weight_0.merge(truth_zero_noice_nhits_lees_7_weight_0, how='left', on='hit_id')#Putting all the data from truth into hits:\n",
    "    def same_layer_filter(hits_new):#Making a same layer filter \n",
    "        hits_long_layer_filtert = hits_new.drop_duplicates(subset = [\"particle_id\",\"volume_id\", \"layer_id\"])\n",
    "        return hits_long_layer_filtert\n",
    "    hits_long_layer_filtert = same_layer_filter(hits_merge)\n",
    "    def Remove_Elements(data, thrsshold):#Making a function that can show the removed particles from same layer filter.\n",
    "        counted = Counter(data)\n",
    "        temp_lst = []\n",
    "        for i in counted:\n",
    "            if counted[i] < thrsshold:\n",
    "                temp_lst.append(i)\n",
    "        res_lst = []\n",
    "        for i in data:\n",
    "            if i not in temp_lst:\n",
    "                res_lst.append(i)\n",
    "        data = [i for i in data if counted[i] >= thrsshold]\n",
    "        return data , temp_lst , res_lst\n",
    "    thrsshold = 4\n",
    "    hits_filtert , particles_nhits_less_4 , particles_nhits_over_4 = Remove_Elements(hits_long_layer_filtert[\"particle_id\"],thrsshold)\n",
    "    hits_merge_filtered = FRBV(hits_long_layer_filtert ,\"particle_id\", particles_nhits_less_4).reset_index().drop(\"index\",axis = 1)#Removing the particles from the same layer filter in hits_merge and particle file\n",
    "    particles_merge_filtered = FRBV(particle_id_with_nhits_lees_7 ,\"particle_id\", particles_nhits_less_4).reset_index().drop(\"index\",axis = 1)#Removing the particles from the same layer filter in hits_merge and particle file\n",
    "    Pt = []\n",
    "    for i in (range(len(particles_merge_filtered))):#Calculating the pt of a particle:\n",
    "        func = (particles_merge_filtered.px[i]**2+particles_merge_filtered.py[i]**2)**(1/2)\n",
    "        Pt.append([particles_merge_filtered.particle_id[i],func])\n",
    "    Pt_1GeV = []\n",
    "    for i in range(len(Pt)):#Making the pt filter loop\n",
    "        if Pt[i][1] >= pt_cut_start and Pt[i][1] <= pt_cut_end: \n",
    "            Pt_1GeV.append(Pt[i])\n",
    "    particles_over_Pt_1GeV = []\n",
    "    for i in range(len(Pt_1GeV)):#Finding the particles from the pt filter \n",
    "        particles_over_Pt_1GeV.append(Pt_1GeV[i][0])\n",
    "    def FCBV(file_name, column_name, list_of_values):# Making a function that keeps values that is in list, and removes the ones that is not. \n",
    "        return file_name[file_name[column_name].isin(list_of_values)]\n",
    "    particles_over_Pt_1GeV_filtered = FCBV(particles_merge_filtered ,\"particle_id\", particles_over_Pt_1GeV).reset_index().drop(\"index\",axis = 1)#Removing the rows from the pt filter. \n",
    "    hits_merge_over_Pt_1GeV_filtered = FCBV(hits_merge_filtered ,\"particle_id\", particles_over_Pt_1GeV).reset_index().drop(\"index\",axis = 1)#Removing the rows from the pt filter. \n",
    "    lenghts = np.array([len(hits),len(hits_volume_filtered),len(hits_zero_noice),len(hits_merge),len(hits_merge_filtered),len(hits_merge_over_Pt_1GeV_filtered)])\n",
    "    lenghts_percent = np.array([len(hits)/len(hits)*100,len(hits_volume_filtered)/len(hits)*100,len(hits_zero_noice)/len(hits)*100,len(hits_merge)/len(hits)*100,len(hits_merge_filtered)/len(hits)*100,len(hits_merge_over_Pt_1GeV_filtered)/len(hits)*100])\n",
    "    return  hits_merge_over_Pt_1GeV_filtered , particles_over_Pt_1GeV_filtered , lenghts , lenghts_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9f2ae5-9518-478a-a19e-d25b1e535982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hits_merge_all_filter = []\n",
    "for i in range(len(hits_all)):\n",
    "    hits_merge_all_filter.append(func_cleaning_data(hits_all[i],particles_all[i],truth_all[i],1,1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9c772-1961-49ae-9f5d-2f10e85507ce",
   "metadata": {},
   "source": [
    "### Looking at the output of the filtering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2bce0d9-c524-49ec-b630-b08ab1eae77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of nodes: 177519 Amount of different particles: 31473\n",
      "Average % of data after all filters 1.61 %\n",
      "Average of data pre event input 109675.0\n",
      "There are between 4 and 10 hits per particle\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lenght_all_nodes_combined = []\n",
    "for i in range(len(hits_merge_all_filter)):\n",
    "    lenght_all_nodes_combined.append(len(hits_merge_all_filter[i][0]))\n",
    "lenght_all_particles_combined = []\n",
    "for i in range(len(hits_merge_all_filter)):\n",
    "    lenght_all_particles_combined.append(len(hits_merge_all_filter[i][1]))\n",
    "print(\"Amount of nodes:\",sum(lenght_all_nodes_combined),\"Amount of different particles:\",sum(lenght_all_particles_combined))\n",
    "\n",
    "data_left = []\n",
    "for i in range(len(hits_merge_all_filter)):\n",
    "    data_left.append(np.round_(hits_merge_all_filter[i][3][5],6))\n",
    "    data_leff_avg = sum(data_left)/len(data_left)\n",
    "data_input = []\n",
    "for i in range(len(hits_merge_all_filter)):\n",
    "    data_input.append(np.round_(hits_merge_all_filter[i][2][0],6))\n",
    "    data_input_avg = sum(data_input)/len(data_input)\n",
    "print(\"Average % of data after all filters\",round(data_leff_avg,2),\"%\")\n",
    "print(\"Average of data pre event input\",round(data_input_avg,0))\n",
    "print(\"There are between\",min((hits_merge_all_filter[0][0][\"particle_id\"]).value_counts()),\"and\",max((hits_merge_all_filter[0][0][\"particle_id\"]).value_counts()),\"hits per particle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3fff2-aabf-4259-a041-6ef0e2783f3c",
   "metadata": {},
   "source": [
    "## Function that findes the edges and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87681f87-cc3e-4f18-9799-af15dd9483fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_and_edges(data,x,y,z):\n",
    "    def transform_and_merge(data,x,y,z):\n",
    "        def car_to_cyl_cood(x,y,z):\n",
    "            r = np.sqrt(x**2 + y**2)\n",
    "            phi = np.arctan2(y,x)\n",
    "            z = z\n",
    "            return r , phi , z \n",
    "        r_hits , phi_hits , z_hits = car_to_cyl_cood(x,y,z)\n",
    "        data[\"phi\"] = phi_hits\n",
    "        data[\"r\"] = r_hits\n",
    "        return data , r_hits , phi_hits , z_hits\n",
    "\n",
    "    hits_merge , r_hits , phi_hits , z_hits = transform_and_merge(data,data.x , data.y , data.z)\n",
    "    def car_to_sphe_cood(r,z):\n",
    "        theta = np.arctan2(r,z)\n",
    "        return theta\n",
    "\n",
    "    theta = car_to_sphe_cood(r_hits,z_hits)\n",
    "    eta = -np.log(np.tan(theta/2))\n",
    "    rr = np.array(hits_merge.r)\n",
    "    zz = np.array(hits_merge.z)\n",
    "    pp = np.array(hits_merge.phi)\n",
    "    dpp = []\n",
    "    node_to_node_feat = []\n",
    "    for i, idx in enumerate(range(len(pp))):\n",
    "        for k, kdx in enumerate(range(len(rr))):\n",
    "            dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
    "            dr = rr[k]-rr[i]\n",
    "            z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n",
    "            if abs(dpdr) < 0.00030035 and abs(z0) < 249 and dr > 0:\n",
    "                dpp.append([(hits_merge.particle_id[i] == hits_merge.particle_id[k])*1,dpdr, idx, kdx, hits_merge.hit_id[idx],\n",
    "                            hits_merge.hit_id[kdx],abs(dr),hits_merge.z[idx],hits_merge.z[kdx],hits_merge.r[idx],hits_merge.r[kdx],hits_merge.phi[idx],\n",
    "                            hits_merge.phi[kdx],hits_merge.x[idx],hits_merge.x[kdx],hits_merge.y[idx],hits_merge.y[kdx]])\n",
    "                node_to_node_feat.append([hits_merge.z[idx],hits_merge.z[kdx],hits_merge.r[idx],hits_merge.r[kdx],hits_merge.phi[idx],hits_merge.phi[kdx]])\n",
    "    dp_over_dr_TF = pd.DataFrame(dpp)\n",
    "    dp_over_dr_TF.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "    dp_over_dr_True = []\n",
    "    for i in range(len(dpp)):\n",
    "        if dpp[i][0] == 1:\n",
    "            dp_over_dr_True.append(dpp[i])\n",
    "    dp_over_dr_True = pd.DataFrame(dp_over_dr_True)\n",
    "    dp_over_dr_True.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "    dp_over_dr_True = dp_over_dr_True.drop_duplicates(subset = \"dpdr\",ignore_index = True)\n",
    "    dp_over_dr_False = []\n",
    "    for i in range(len(dpp)):\n",
    "        if dpp[i][0] == 0:\n",
    "            dp_over_dr_False.append(dpp[i])\n",
    "    dp_over_dr_False = pd.DataFrame(dp_over_dr_False)\n",
    "    dp_over_dr_False.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "    dp_over_dr_False = dp_over_dr_False.drop_duplicates(subset = \"dpdr\",ignore_index = True)\n",
    "    #Making the datafram that contains the nodes ids and the nodes features. \n",
    "    nodes_id_plus_features = hits_merge[[\"hit_id\",\"z\",\"r\",\"phi\"]].copy()\n",
    "    return nodes_id_plus_features , dp_over_dr_True , dp_over_dr_False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b75e09-0310-4836-aa0f-8e6b03fa6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_4452/3205670029.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_4452/3205670029.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_4452/3205670029.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  dpdr = (pp[k]- pp[i])/(rr[k]-rr[i])\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp/ipykernel_4452/3205670029.py:29: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  z0 =  zz[i] - (rr[i]*(zz[k]-zz[i])/(rr[k]-rr[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hits_merge_all = [nodes_and_edges(hits_merge_all_filter[i][0] , hits_merge_all_filter[i][0].x , hits_merge_all_filter[i][0].y , hits_merge_all_filter[i][0].z) for i in range(len(hits_merge_all_filter))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208863c-dde7-4792-ab76-0ffda67f7705",
   "metadata": {},
   "source": [
    "### Makeing functions that removes duplicates of the output data of the edges and nodes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ec6ce59-bd03-4e43-adae-755df141b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of nodes: 177519 Amount of different particles: 31473\n",
      "Output 145180 the same as nodes-particles 146046\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def REWMH(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_True = data[1].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_True = data_sorted_True.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    data_sorted_False = data[2].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_False = data_sorted_False.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    True_and_false = data_True.append(data_False, ignore_index=True)\n",
    "    return True_and_false\n",
    "\n",
    "def REWMHT(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_True = data[1].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_True = data_sorted_True.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    return data_True\n",
    "\n",
    "def REWMHF(data):#removing_edges_with_to_many_hits_ids\n",
    "    data_sorted_False = data[2].sort_values(by = [\"hit_id_node1\",\"dr\"],ascending=True)\n",
    "    data_False = data_sorted_False.drop_duplicates(subset = \"hit_id_node1\",ignore_index = True)\n",
    "    return data_False\n",
    "\n",
    "All_edges_TF_095 = [REWMH(hits_merge_all[:math.floor(len(hits_merge_all)*0.95)][i]) for i in range(math.floor(len(hits_merge_all)*0.95))] \n",
    "All_edges_TF_005 = [REWMH(hits_merge_all[-math.ceil(len(hits_merge_all)*0.05):][i]) for i in range(math.ceil(len(hits_merge_all)*0.05))]\n",
    "All_edges_TF_095_combined = pd.concat(All_edges_TF_095, ignore_index=True)\n",
    "All_edges_TF_005_combined = pd.concat(All_edges_TF_005, ignore_index=True)\n",
    "\n",
    "All_edges_TF = [REWMH(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "All_edges_TF_combined = pd.concat(All_edges_TF, ignore_index=True)\n",
    "\n",
    "All_edges_T = [REWMHT(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "All_edges_F = [REWMHF(hits_merge_all[i]) for i in range(len(hits_merge_all))] \n",
    "All_edges_T_combined = pd.concat(All_edges_T, ignore_index=True)\n",
    "All_edges_F_combined = pd.concat(All_edges_F, ignore_index=True)\n",
    "\n",
    "print(\"Amount of nodes:\",sum(lenght_all_nodes_combined),\"Amount of different particles:\",sum(lenght_all_particles_combined))\n",
    "print(\"Output\",len(All_edges_T_combined),\"the same as nodes-particles\",sum(lenght_all_nodes_combined)-sum(lenght_all_particles_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370df3e-0375-4e45-a913-5c084feea5d1",
   "metadata": {},
   "source": [
    "## Saving the edges and nodes to be used in MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fedb075-7f33-4858-aa12-7c3a4fb4b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_edges_TF_combined = pd.DataFrame(All_edges_TF_combined)\n",
    "All_edges_TF_combined.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "All_edges_TF_combined.to_csv(\"Events_100_edges_TF_100events_combined_pt1_125.csv\",index=False)\n",
    "\n",
    "All_edges_TF_095_combined = pd.DataFrame(All_edges_TF_095_combined)\n",
    "All_edges_TF_095_combined.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "All_edges_TF_095_combined.to_csv(\"Events_100_edges_TF_95events_combined_pt1_125.csv\",index=False)\n",
    "\n",
    "All_edges_TF_005_combined = pd.DataFrame(All_edges_TF_005_combined)\n",
    "All_edges_TF_005_combined.columns = ['Y_k' , \"dpdr\" , 'node_1', 'node_2',\"hit_id_node1\",\"hit_id_node2\",\"dr\",\"z_node1\",\"z_node2\",\"r_node1\",\"r_node2\",\"phi_node1\",\"phi_node2\",\n",
    "                            \"x_node1\",\"x_node2\",\"y_node1\",\"y_node2\"]\n",
    "All_edges_TF_005_combined.to_csv(\"Events_100_edges_TF_5events_combined_pt1_125.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
